{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/getnetdemil/Deep-Learning-workshop/blob/master/Visual_Saliency_%5Bcloze%5D.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hLqsB40IW1ad"
      },
      "source": [
        "# Training a saliency prediction network\n",
        "\n",
        "[Kevin McGuinness](http://www.eeng.dcu.ie/~mcguinne/), School of Electronic Engineering, [Dublin City University](https://www.dcu.ie/)\n",
        "\n",
        "[Insight Centre for Data Analytics](https://www.insight-centre.org/)\n",
        "\n",
        "---\n",
        "\n",
        "This lab will illustrate how to specify and train a visual saliency prediction network. We will use a modified [U-Net architecture](https://lmb.informatik.uni-freiburg.de/people/ronneber/u-net/), and train on a downsampled version of the [SALICON](http://salicon.net/) dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FGmS80CT8aCw"
      },
      "source": [
        "## Instructions\n",
        "\n",
        "Anywhere you see a **???** in the code below, fill in in with the correct code."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0LSHsyLHVbzM"
      },
      "source": [
        "## Enable GPU acceleration\n",
        "\n",
        "Open to the Edit menu and select *Notebook settings* and then select *GPU* under hardware accelerator.\n",
        "\n",
        "Change the line below to ``device = 'cpu'`` to run on the CPU instead."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xBIxX5Pi2HxJ"
      },
      "source": [
        "device = 'cuda'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s9L0lUe_v6EZ"
      },
      "source": [
        "## Imports\n",
        "\n",
        "\n",
        "Find the PyTorch docs at https://pytorch.org/docs/stable/index.html\n",
        "\n",
        "Tutorials: https://pytorch.org/tutorials/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jDG8QxnYyqrg",
        "outputId": "8a568104-3518-4f12-8141-756d2911dcd2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%pylab inline"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Populating the interactive namespace from numpy and matplotlib\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zxHHwHj-vecX"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "import os\n",
        "import itertools\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.transforms.functional as tF\n",
        "\n",
        "from torchvision.datasets.utils import list_dir, list_files\n",
        "from torchvision.datasets.folder import pil_loader\n",
        "from torchvision.datasets.utils import download_file_from_google_drive\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "from google.colab import widgets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rLhukfIxv-lM"
      },
      "source": [
        "## Dataset download\n",
        "\n",
        "Mini-SALICON is a downsampled version of the [SALICON](http://salicon.net/) dataset that I created to be compact enough (~140MB) for demonstration purposes. You can download it using tyis [Google Drive link](https://drive.google.com/file/d/19X4y27WGZY4iO3p2bd38l46lD9uW72mU/view?usp=sharing). Here I use a utility function from PyTorch for downloading from Google Drive."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bamvM06MwnZX",
        "outputId": "3ccd5bcd-c8bd-444c-89ef-dc7f3d58b934",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "download_file_from_google_drive('19X4y27WGZY4iO3p2bd38l46lD9uW72mU', 'data', 'salicon-mini.tar')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "147783680it [00:02, 72900326.63it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C55wnom8fBr9"
      },
      "source": [
        "Extract the tar file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mQFhjPEvwhDP"
      },
      "source": [
        "!cd data && tar xf salicon-mini.tar"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z4stakSpfFAR"
      },
      "source": [
        "Clean up"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V0cXXkmRw94G"
      },
      "source": [
        "!rm data/salicon-mini.tar"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t-Jkf19GxK5L",
        "outputId": "7623cf2e-f0b1-450c-9a5e-de1ceaf585c2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!ls data/salicon-mini"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "make.py  train\tvalid\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "el9rqvyk0DZm"
      },
      "source": [
        "!mkdir -p checkpoints"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eLIH4jVUwB5J"
      },
      "source": [
        "## Dataset class\n",
        "\n",
        "We define a dataset class following the standard pattern used in PyTorch. The `__getitem__` method returns an (image, saliency map) pair."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BZ6yEWjOxNW6"
      },
      "source": [
        "class SaliconMini(Dataset):\n",
        "    def __init__(self,\n",
        "                 root='data/salicon-mini',\n",
        "                 subset='train',\n",
        "                 im_transform=None,\n",
        "                 sm_transform=None,\n",
        "                 loader=pil_loader):\n",
        "        self.root = root\n",
        "        self.im_transform = im_transform\n",
        "        self.sm_transform = sm_transform\n",
        "        self.loader = loader\n",
        "\n",
        "        # locate the subset\n",
        "        self.subdir = os.path.join(self.root, subset)\n",
        "\n",
        "        # the image and saliency map subfolders\n",
        "        self.im_dir = os.path.join(self.subdir, 'im')\n",
        "        self.sm_dir = os.path.join(self.subdir, 'sm')\n",
        "\n",
        "        # list the images in the image folder\n",
        "        self.file_list = list_files(self.im_dir, '.jpg', prefix=False)\n",
        "        self.file_list.sort()\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "\n",
        "        # get the image and saliency map filenames\n",
        "        im_filename = self.file_list[index]\n",
        "        sm_filename = im_filename[:-4] + '.png'\n",
        "\n",
        "        # get the full image path\n",
        "        im_filename = os.path.join(self.im_dir, im_filename)\n",
        "\n",
        "        # get the full saliency map path\n",
        "        #sm_filename = ???\n",
        "        sm_filename = os.path.join(self.sm_dir, sm_filename)\n",
        "\n",
        "        # load the image\n",
        "        im = self.loader(im_filename)\n",
        "\n",
        "        # load the saliency map\n",
        "        #sm = ???\n",
        "        sm = self.loader(sm_filename)\n",
        "\n",
        "        # apply image transform if specified\n",
        "        if self.im_transform is not None:\n",
        "            im = self.im_transform(im)\n",
        "\n",
        "        # apply saliency map transform if specified\n",
        "        if self.sm_transform is not None:\n",
        "            #sm = ???\n",
        "            sm = self.sm_transform(sm)\n",
        "\n",
        "        return im, sm\n",
        "\n",
        "    def __len__(self):\n",
        "        # return the number of entries in the dataset\n",
        "        return len(self.file_list) #???\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lh8adog-wJkA"
      },
      "source": [
        "## Model architecture\n",
        "\n",
        "This is a version of the popular U-Net architecure by [Olaf Ronneberger, Philipp Fischer, Thomas Brox](https://arxiv.org/abs/1505.04597). The architecure was originally developed for medical image segmentation but has been found to be suited to a variety of tasks. The skip connections help both with achieving good spatial detail and avoiding gradient vanishing.\n",
        "\n",
        "\n",
        "![UNet architecure](https://lmb.informatik.uni-freiburg.de/people/ronneber/u-net/u-net-architecture.png)\n",
        "\n",
        "Here we repurpose the network for saliency detection. The output layer is a single channel sigmoid the same size as the input layer.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QgzFtSvq14nR"
      },
      "source": [
        "\n",
        "class SalUNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SalUNet, self).__init__()\n",
        "\n",
        "        # resolution: 128 x 96\n",
        "        self.conv1_e1 = ConvAct(3, 64)\n",
        "        self.conv1_e2 = ConvAct(64, 64)\n",
        "        self.down1 = nn.MaxPool2d(2, 2)\n",
        "\n",
        "        # resolution: 64 x 48\n",
        "        self.conv2_e1 = ConvAct(64, 128)\n",
        "        self.conv2_e2 = ConvAct(128, 128)\n",
        "        self.down2 = nn.MaxPool2d(2, 2)\n",
        "\n",
        "        # resolution: 32 x 24\n",
        "        self.conv3_e1 = ConvAct(128, 256)\n",
        "        self.conv3_e2 = ConvAct(256, 256)\n",
        "        self.down3 = nn.MaxPool2d(2, 2)\n",
        "\n",
        "        # resolution: 16 x 12\n",
        "        self.conv4_e1 = ConvAct(256, 512)\n",
        "        self.conv4_e2 = ConvAct(512, 512) #???\n",
        "        self.down4 = nn.MaxPool2d(2, 2)\n",
        "\n",
        "        # resolution: 8 x 6\n",
        "        self.conv5_1 = ConvAct(512, 1024)\n",
        "        self.conv5_2 = ConvAct(1024, 1024)\n",
        "\n",
        "        # resolution: 16 x 12\n",
        "        self.up4 = nn.ConvTranspose2d(1024, 512, 3, 2, 1, 1)\n",
        "        self.conv4_d1 = ConvAct(1024, 512)\n",
        "        self.conv4_d2 = ConvAct(512, 512)\n",
        "\n",
        "        # resolution: 32 x 24\n",
        "        self.up3 = nn.ConvTranspose2d(512, 256, 3, 2, 1, 1)\n",
        "        self.conv3_d1 = ConvAct(512, 256)\n",
        "        self.conv3_d2 = ConvAct(256, 256)\n",
        "\n",
        "        # resolution: 64 x 48\n",
        "        self.up2 = nn.ConvTranspose2d(256, 128, 3, 2, 1, 1)\n",
        "        self.conv2_d1 = ConvAct(256, 128)\n",
        "        self.conv2_d2 = ConvAct(128, 128) #???\n",
        "\n",
        "        # resolution: 128 x 96\n",
        "        self.up1 = nn.ConvTranspose2d(128, 64, 3, 2, 1, 1)\n",
        "        self.conv1_d1 = ConvAct(128, 64)\n",
        "        self.conv1_d2 = ConvAct(64, 64)\n",
        "\n",
        "        self.output_layer = ConvAct(64, 1, 1, activation='sigmoid')\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        # level 1 - encode\n",
        "        x = self.conv1_e1(x)\n",
        "        x = out1 = self.conv1_e2(x)\n",
        "        x = self.down1(x)\n",
        "\n",
        "        # level 2 - encode\n",
        "        x = self.conv2_e1(x)\n",
        "        x = out2 = self.conv2_e2(x)\n",
        "        x = self.down2(x)\n",
        "\n",
        "        # level 3 - encode\n",
        "        x = self.conv3_e1(x) #???\n",
        "        x = out3 = self.conv3_e2(x)\n",
        "        x = self.down3(x)\n",
        "\n",
        "        # level 4 - encode\n",
        "        x = self.conv4_e1(x)\n",
        "        x = out4 = self.conv4_e2(x)\n",
        "        x = self.down4(x) #???\n",
        "\n",
        "        # level 5 - encode/decode\n",
        "        x = self.conv5_1(x)\n",
        "        x = self.conv5_2(x)\n",
        "\n",
        "        # level 4 - decode\n",
        "        x = self.up4(x)\n",
        "        x = torch.cat((x, out4), dim=1)\n",
        "        x = self.conv4_d1(x)\n",
        "        x = self.conv4_d2(x)\n",
        "\n",
        "        # level 3 - decode\n",
        "        x = self.up3(x)\n",
        "        x = torch.cat((x, out3), dim=1)\n",
        "        x = self.conv3_d1(x)\n",
        "        x = self.conv3_d2(x) #???\n",
        "\n",
        "        # level 2 - decode\n",
        "        x = self.up2(x)\n",
        "        x = torch.cat((x, out2), dim=1)\n",
        "        x = self.conv2_d1(x)\n",
        "        x = self.conv2_d2(x)\n",
        "\n",
        "        # level 1 - decode\n",
        "        x = self.up1(x)\n",
        "        x = torch.cat((x, out1), dim=1) #???\n",
        "        x = self.conv1_d1(x)\n",
        "        x = self.conv1_d2(x)\n",
        "\n",
        "        # make predictions\n",
        "        x = self.output_layer(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "_named_activations = {\n",
        "    'relu': nn.ReLU,\n",
        "    'sigmoid': nn.Sigmoid,\n",
        "    'prelu': nn.PReLU,\n",
        "    'leakyrelu': nn.LeakyReLU,\n",
        "    'elu': nn.ELU\n",
        "}\n",
        "\n",
        "\n",
        "def same_padding(kernel, dilation):\n",
        "    return int((kernel + (kernel - 1) * (dilation - 1) - 1) / 2)\n",
        "\n",
        "\n",
        "def get_activation_by_name(name, **kwargs):\n",
        "    return _named_activations[name](**kwargs)\n",
        "\n",
        "\n",
        "class ConvAct(nn.Module):\n",
        "    \"\"\"Conv->Activation\"\"\"\n",
        "    def __init__(\n",
        "            self, in_channels, out_channels,\n",
        "            kernel=3, dilation=1, activation='relu'):\n",
        "        super(ConvAct, self).__init__()\n",
        "        padding = same_padding(kernel, dilation)\n",
        "        self.conv = nn.Conv2d(\n",
        "            in_channels, out_channels, kernel,\n",
        "            padding=padding,\n",
        "            dilation=dilation)\n",
        "        self.activation = get_activation_by_name(activation)\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        self.conv.reset_parameters()\n",
        "        nn.init.xavier_uniform_(self.conv.weight)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.activation(self.conv(x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eN3lOtlrwXQN"
      },
      "source": [
        "## Utilities\n",
        "\n",
        "Usually when training a deep learning model, you want to keep track of the loss. However, the **loss on individual batches is quite noisy**, so I usually use an **exponentially decayed moving average** to smooth out individual fluctuations in each batch and make general trends more easy to see.\n",
        "\n",
        "At validation time you usually want to find the average loss over the full dataset, so a running average (cumulative moving average) is more appropriate.\n",
        "\n",
        "Here we define two classes to take care of this."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zcHx5zHO8Que"
      },
      "source": [
        "class AverageBase(object):\n",
        "\n",
        "    def __init__(self, value=0):\n",
        "        self.value = float(value) if value is not None else None\n",
        "\n",
        "    def __str__(self):\n",
        "        return str(round(self.value, 4))\n",
        "\n",
        "    def __repr__(self):\n",
        "        return self.value\n",
        "\n",
        "    def __format__(self, fmt):\n",
        "        return self.value.__format__(fmt)\n",
        "\n",
        "    def __float__(self):\n",
        "        return self.value\n",
        "\n",
        "\n",
        "class RunningAverage(AverageBase):\n",
        "    \"\"\"\n",
        "    Keeps track of a cumulative moving average (CMA).\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, value=0, count=0):\n",
        "        super(RunningAverage, self).__init__(value)\n",
        "        self.count = count\n",
        "\n",
        "    def update(self, value):\n",
        "        self.value = (self.value * self.count + float(value))\n",
        "        self.count += 1\n",
        "        self.value /= self.count\n",
        "        return self.value\n",
        "\n",
        "\n",
        "class MovingAverage(AverageBase):\n",
        "    \"\"\"\n",
        "    An exponentially decaying moving average (EMA).\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, alpha=0.99):\n",
        "        super(MovingAverage, self).__init__(None)\n",
        "        self.alpha = alpha\n",
        "\n",
        "    def update(self, value):\n",
        "        if self.value is None:\n",
        "            self.value = float(value)\n",
        "        else:\n",
        "            self.value = self.alpha * self.value + (1 - self.alpha) * float(value)\n",
        "        return self.value"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G3lALNaQwoHR"
      },
      "source": [
        "## Create datasets and loaders\n",
        "\n",
        "Here we don't do any random horizontal flips becasue we would need to be careful to apply the same flip to the saliency map. It would be a nice exercise to modify the data loaders to incorporate this functionality."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fyxZk1z0yYOI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dfb91746-ff90-4989-f332-00400770bd0b"
      },
      "source": [
        "im_tf = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
        "])\n",
        "\n",
        "sm_tf = transforms.Compose([\n",
        "    transforms.Grayscale(),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "# create training and validation datasets\n",
        "train_set = SaliconMini(subset='train', im_transform=im_tf, sm_transform=sm_tf)\n",
        "valid_set = SaliconMini(subset='valid', im_transform=im_tf, sm_transform=sm_tf)\n",
        "print(len(train_set), len(valid_set))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "9500 500\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OYRuFZ3myvBX"
      },
      "source": [
        "# create a data loader with a batch size of 16 and shuffle enabled\n",
        "train_loader = DataLoader(train_set, batch_size=16, shuffle=True, pin_memory=True)\n",
        "\n",
        "# create a validation loader with smaller batch size (10) and shuffle disabled\n",
        "#valid_loader = ???\n",
        "valid_loader = DataLoader(valid_set, batch_size=10, shuffle=False, pin_memory=True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5CjBVoL7wwBR"
      },
      "source": [
        "## Instantiate model\n",
        "\n",
        "Create an instance of the model and move it (memory and operations) to the CUDA device."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rJ5Qw0pq38NC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be3f5fbb-edf7-4ebf-eb94-4bd7758d7bdb"
      },
      "source": [
        "import torch.nn as nn\n",
        "from torchvision.models import resnet18\n",
        "# instantiate our model and move it to the GPU\n",
        "#model = ???\n",
        "model = resnet18(pretrained=False)\n",
        "model.to(device);"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K89XZ2Z7w1Ln"
      },
      "source": [
        "## Setup objective and optimizer\n",
        "\n",
        "The saliency prediction model essentially outputs a probability for each pixel that it will be attended to. The total **binary cross entropy** is an appropriate loss function to use here.\n",
        "\n",
        "We will use **Adam** as the optimizer with a learning rate of $10^{-4}$. SGD can probably do better than Adam in the long run, but Adam gets better results than SGD when the training time is restricted to a few epochs. Since we do not want to spend a very long time training in this lab, let's use Adam. See [Sebastian Ruder's excellent blog post](https://ruder.io/optimizing-gradient-descent/) for more information about Adam and other optimizers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5DSPqCSww4Rw"
      },
      "source": [
        "# Create the optimization criterion (loss)\n",
        "criterion = nn.BCELoss()\n",
        "\n",
        "# Create an Adam optimizer with learning rate of 0.0001\n",
        "#optimizer = optim.Adam(???, lr=1e-4)\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eS50FtxjEbNW"
      },
      "source": [
        "## Checkpointing\n",
        "\n",
        "The following helpers functions are used for loading and saving snapshots of the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gKkqER61EeO5"
      },
      "source": [
        "def save_checkpoint(optimizer, model, epoch, filename):\n",
        "    checkpoint_dict = {\n",
        "        'optimizer': optimizer.state_dict(),\n",
        "        'model': model.state_dict(),\n",
        "        'epoch': epoch\n",
        "    }\n",
        "    torch.save(checkpoint_dict, filename)\n",
        "\n",
        "\n",
        "def load_checkpoint(optimizer, model, filename):\n",
        "    checkpoint_dict = torch.load(filename)\n",
        "    epoch = checkpoint_dict['epoch']\n",
        "    model.load_state_dict(checkpoint_dict['model'])\n",
        "    if optimizer is not None:\n",
        "        optimizer.load_state_dict(checkpoint_dict['optimizer'])\n",
        "    return epoch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FunW3QQ3xA0h"
      },
      "source": [
        "## Train helpers\n",
        "\n",
        "Here we define some helper functions abstract some of the training. Many of these are defined as Python generators, which makes it easy to do something with the loss at the end of each iteration."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hrMCksVOxC84"
      },
      "source": [
        "def train_for_epoch(loader, model, optimizer, criterion):\n",
        "    n_steps = len(train_loader)\n",
        "\n",
        "    # put the model in train mode\n",
        "    #model.???\n",
        "    model.train()\n",
        "\n",
        "    # iterate over batches\n",
        "    for step, (batch, targets) in enumerate(loader):\n",
        "\n",
        "        # Move the training data to the GPU\n",
        "        batch = batch.to(device)\n",
        "        #targets = ???\n",
        "        targets = targets.to(device)\n",
        "\n",
        "        # clear previous gradient computation\n",
        "        #optimizer.???\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward propagation\n",
        "        #predictions = ???\n",
        "        predictions = model(batch)\n",
        "\n",
        "        # calculate the loss\n",
        "        #loss = ???\n",
        "        loss = criterion(predictions, targets)\n",
        "\n",
        "        # backpropagate to compute gradients\n",
        "        #loss.???\n",
        "        loss.backward()\n",
        "\n",
        "        # update model weights\n",
        "        #optimizer.???\n",
        "        optimizer.step()\n",
        "\n",
        "        yield step, n_steps, float(loss)\n",
        "\n",
        "\n",
        "def validate(loader, model, optimizer, criterion):\n",
        "\n",
        "    # put the model in eval mode\n",
        "    #model.???\n",
        "    model.eval()\n",
        "\n",
        "    # use a running average to keep track of the average loss\n",
        "    valid_loss = RunningAverage(count=len(loader))\n",
        "\n",
        "    # We don't need gradients for validation, so wrap in\n",
        "    # no_grad to save memory\n",
        "    with torch.no_grad():\n",
        "\n",
        "        for step, (batch, targets) in enumerate(loader):\n",
        "            # Move the training batch to the GPU\n",
        "            batch = batch.to(device)\n",
        "            targets = targets.to(device)\n",
        "\n",
        "            # forward propagation\n",
        "            #predictions = ???\n",
        "            predictions = model(batch)\n",
        "\n",
        "            # criterion = nn.CrossEntropyLoss()\n",
        "            # calculate the loss\n",
        "            loss = criterion(predictions, targets)\n",
        "\n",
        "            # update running loss value\n",
        "            valid_loss.update(loss)\n",
        "\n",
        "    return valid_loss.value\n",
        "\n",
        "\n",
        "def monitor_progress(train_generator):\n",
        "    start_time = time.time()\n",
        "    for step, n_steps, loss in train_generator:\n",
        "        elapsed = int(time.time() - start_time)\n",
        "        print(f'\\rBatch {step+1:03d}/{n_steps}  loss: {loss:0.4f}  elapsed: {elapsed}s',\n",
        "              end='', flush=True)\n",
        "    print()\n",
        "    yield step, n_steps, loss\n",
        "\n",
        "\n",
        "def track_running_average_loss(train_generator):\n",
        "    average_loss = MovingAverage()\n",
        "    for step, n_steps, loss in train_generator:\n",
        "        average_loss.update(loss)\n",
        "        yield step, n_steps, average_loss.value\n",
        "\n",
        "\n",
        "def run_train_generator(train_generator):\n",
        "    for step, n_steps, loss in train_generator:\n",
        "        pass\n",
        "    return loss\n",
        "\n",
        "\n",
        "def show_examples(dataset, model, count=5, size=(3.3, 2.5)):\n",
        "    loader = DataLoader(dataset, batch_size=1, shuffle=False)\n",
        "\n",
        "    # same mean and standard deviation that were applied in the transform\n",
        "    pix_mean = torch.as_tensor([0.4914, 0.4822, 0.4465], dtype=torch.float32, device=device)\n",
        "    pix_std = torch.as_tensor([0.2023, 0.1994, 0.2010], dtype=torch.float32, device=device)\n",
        "\n",
        "    # put model in eval mode\n",
        "    model.eval()\n",
        "\n",
        "    # create a colab grid\n",
        "    grid = widgets.Grid(3, count)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, (batch, targets) in enumerate(itertools.islice(loader, count)):\n",
        "            batch = batch.to(device)\n",
        "            targets = targets.to(device)\n",
        "            predictions = model(batch)\n",
        "\n",
        "            # unnormalize image\n",
        "            image = batch[0].mul(pix_std[:, None, None]).add(pix_mean[:, None, None])\n",
        "            image.clamp_(0,1)\n",
        "\n",
        "            # convert to PIL images\n",
        "            image = tF.to_pil_image(image.cpu())\n",
        "            target = tF.to_pil_image(targets.cpu()[0,0])\n",
        "            y_pred = tF.to_pil_image(predictions.cpu()[0,0])\n",
        "\n",
        "            # display\n",
        "            with grid.output_to(0,i):\n",
        "                grid.clear_cell()\n",
        "                plt.figure(figsize=size)\n",
        "                plt.imshow(image)\n",
        "            with grid.output_to(1,i):\n",
        "                grid.clear_cell()\n",
        "                plt.figure(figsize=size)\n",
        "                plt.imshow(target)\n",
        "            with grid.output_to(2,i):\n",
        "                grid.clear_cell()\n",
        "                plt.figure(figsize=size)\n",
        "                plt.imshow(y_pred)\n",
        "\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SR-kxAzHw_FA"
      },
      "source": [
        "## Train the model\n",
        "First, let's define a function that uses the above utilities to train the model for a specified number of epochs.\n",
        "Note: we are training **from scratch**: no pre-trained weights from imagenet are used here!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3z20QHG29yd4"
      },
      "source": [
        "def train(first_epoch, n_epochs):\n",
        "\n",
        "    # keep track of losses\n",
        "    train_losses, valid_losses = [], []\n",
        "\n",
        "    for epoch in range(first_epoch, n_epochs + first_epoch):\n",
        "\n",
        "        # train\n",
        "        train_generator = train_for_epoch(train_loader, model, optimizer, criterion)\n",
        "        train_generator = track_running_average_loss(train_generator)\n",
        "        train_generator = monitor_progress(train_generator)\n",
        "        train_loss = run_train_generator(train_generator)\n",
        "        train_losses.append(train_loss)\n",
        "\n",
        "        # validate\n",
        "        valid_loss = validate(valid_loader, model, optimizer, criterion)\n",
        "        valid_losses.append(valid_loss)\n",
        "        print('Epoch {} validation loss: {:.5f}'.format(epoch, valid_loss))\n",
        "\n",
        "        # Save a checkpoint\n",
        "        checkpoint_filename = 'checkpoints/salunet-{:03d}.pkl'.format(epoch)\n",
        "        save_checkpoint(optimizer, model, epoch, checkpoint_filename)\n",
        "\n",
        "    return train_losses, valid_losses"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aZcYHgX-Sb1p"
      },
      "source": [
        "Time to train the model. Each epoch here takes around between 1 and 4 minutes depending on the GPU you are allocated."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2wUsYwukztwj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 328
        },
        "outputId": "cf56a5ba-b4a4-427a-e1a0-55a9e4f76dd6"
      },
      "source": [
        "train_losses, valid_losses = train(first_epoch=1, n_epochs=5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-8d42b8e8bd4a>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_losses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfirst_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-18-593991b4d821>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(first_epoch, n_epochs)\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mtrain_generator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrack_running_average_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_generator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mtrain_generator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmonitor_progress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_generator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_train_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_generator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0mtrain_losses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-17-1bdebc9845ba>\u001b[0m in \u001b[0;36mrun_train_generator\u001b[0;34m(train_generator)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mrun_train_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_generator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_steps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_generator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m         \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-17-1bdebc9845ba>\u001b[0m in \u001b[0;36mmonitor_progress\u001b[0;34m(train_generator)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmonitor_progress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_generator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_steps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_generator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m         \u001b[0melapsed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         print(f'\\rBatch {step+1:03d}/{n_steps}  loss: {loss:0.4f}  elapsed: {elapsed}s',\n",
            "\u001b[0;32m<ipython-input-17-1bdebc9845ba>\u001b[0m in \u001b[0;36mtrack_running_average_loss\u001b[0;34m(train_generator)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtrack_running_average_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_generator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0maverage_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMovingAverage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_steps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_generator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m         \u001b[0maverage_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0;32myield\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_steps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-17-1bdebc9845ba>\u001b[0m in \u001b[0;36mtrain_for_epoch\u001b[0;34m(loader, model, optimizer, criterion)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;31m# calculate the loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;31m#loss = ???\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;31m# backpropagate to compute gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    617\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    618\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 619\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary_cross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    620\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mbinary_cross_entropy\u001b[0;34m(input, target, weight, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   3087\u001b[0m         \u001b[0mreduction_enum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3088\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3089\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m   3090\u001b[0m             \u001b[0;34m\"Using a target size ({}) that is different to the input size ({}) is deprecated. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3091\u001b[0m             \u001b[0;34m\"Please ensure they have the same size.\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Using a target size (torch.Size([16, 1, 96, 128])) that is different to the input size (torch.Size([16, 1000])) is deprecated. Please ensure they have the same size."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vOViUcquSJYe"
      },
      "source": [
        "## Show some examples\n",
        "\n",
        "Display some examples from the validation set. The top row shows the image, the middle row is the ground truth, and the bottom row is the predicted saliency map."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UC_aeVQ636bq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "outputId": "fc08c0d1-0e41-4a8e-fdae-d788461042dc"
      },
      "source": [
        "show_examples(valid_set, model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "       table#id4, #id4 > tbody > tr > th, #id4 > tbody > tr > td {\n",
              "         border: 1px solid lightgray;\n",
              "         border-collapse:collapse;\n",
              "         \n",
              "        }</style>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<table id=id4><tr><td id=id4-0-0></td><td id=id4-0-1></td><td id=id4-0-2></td><td id=id4-0-3></td><td id=id4-0-4></td></tr><tr><td id=id4-1-0></td><td id=id4-1-1></td><td id=id4-1-2></td><td id=id4-1-3></td><td id=id4-1-4></td></tr><tr><td id=id4-2-0></td><td id=id4-2-1></td><td id=id4-2-2></td><td id=id4-2-3></td><td id=id4-2-4></td></tr></table>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-d70540fb097a>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mshow_examples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-17-1bdebc9845ba>\u001b[0m in \u001b[0;36mshow_examples\u001b[0;34m(dataset, model, count, size)\u001b[0m\n\u001b[1;32m    118\u001b[0m             \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_pil_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_pil_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m             \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_pil_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m             \u001b[0;31m# display\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torchvision/transforms/functional.py\u001b[0m in \u001b[0;36mto_pil_image\u001b[0;34m(pic, mode)\u001b[0m\n\u001b[1;32m    264\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndimension\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 266\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"pic should be 2/3 dimensional. Got {pic.ndimension()} dimensions.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mpic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndimension\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: pic should be 2/3 dimensional. Got 0 dimensions."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5z44YEizYRxj"
      },
      "source": [
        "## Train for longer\n",
        "\n",
        "Looks like we've started to learn to predict visual saliency! Training for another few epochs can improve the model. Let's train for another 5 epochs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xV2zZpAvK2HX"
      },
      "source": [
        "#train_losses, valid_losses = ???\n",
        "train_losses, valid_losses = train(first_epoch=6, n_epochs=5)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5F1KPkQJeeln"
      },
      "source": [
        "And show some examples from the validation set:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m-FhM4Tnkj-0"
      },
      "source": [
        "# ???\n",
        "show_examples(valid_set, model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R5ch2kKyetbp"
      },
      "source": [
        "Notice how the predictions are a little smoother and more focused than before."
      ]
    }
  ]
}